{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Democratize Documentation Summarization in Media and Entertainment with Hugging Face Transformers on Amazon SageMaker\n",
    "### Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Model Setup](#Model-Setup)\n",
    "3. [Model Deployment](#Model-Deployment)\n",
    "4. [Test Model Endpoint](#Test-the-endpoint)\n",
    "5. [Clean Up](#Clean-Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook we'll go through an example of deploying a document summarization model from [HuggingFace Hub](https://huggingface.co/models). This model will take as input a long text document and will output a concise summary. We will deploy this model on an [asynchronous endpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html) as most summarization use case do not require low latency making the asynchronous endpoint the ideal choice. This notebook was tested on SageMaker Studio running on the Python Data Science kernel and a ml.t3.medium instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, lets first install the correct dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uqq sagemaker==2.59.7\n",
    "%pip install -Uqq boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "In this section we will configure the model package that we will deploy. The package includes a `distilbart-cnn-12-6` pretrained model and the source code used to serve it. HuggingFace provides a convinient python function for pulling a model from it's hub. We can invoke this function directly from our inference script, however this is not a good practice as it itroduces a potential point of failure if for some reason our endpoint is not able to communicate with HuggingFace Hub during the deployment or autoscalling operation. In this section we will therefore use git-lfs to download a pretrained model from the Hub and host it in our own S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install git LFS and clone the distilbart model from HF Hub\n",
    "! curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh |  bash\n",
    "! apt-get install git-lfs\n",
    "! git clone https://huggingface.co/sshleifer/distilbart-cnn-12-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the initial model artifact tar file\n",
    "! cd distilbart-cnn-12-6 && tar --exclude=\".*\" -cvf  model.tar * && mv model.tar ../model.tar\n",
    "! rm -r distilbart-cnn-12-6/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()      # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()         # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()             # bucket to house artifacts\n",
    "key_prefix = \"hugginface_summarization\"    # folder within bucket where all artifacts will go\n",
    "\n",
    "region = sess._region_name\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"code\"):\n",
    "    os.makedirs(\"code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is our inference code. The `model_fn` is responsible for loading the model while the `transform_fn` will be responsible for the actual inference logic. For simplicity, we'll use the HuggingFace pipeline abstraction to combine the tokenizer with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/sum_entrypoint.py\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \n",
    "    tokenizer = BartTokenizer.from_pretrained(model_dir)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_dir)\n",
    "    nlp=pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    return nlp\n",
    "\n",
    "\n",
    "def transform_fn(nlp, request_body, input_content_type, output_content_type=\"text/csv\"):\n",
    "    \n",
    "    if input_content_type == \"text/csv\":\n",
    "        result = nlp(request_body, truncation=True)[0]\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"content type not supported\")\n",
    "    \n",
    "    return json.dumps(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the inference code to our tar model package and compress it using gz to produce the final `model.tar.gz` artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the inference code to the model artifact, zip the archive, delete the initial tar. The compression step could take 5 to 10 minutes as this is a large model\n",
    "! tar rvf  model.tar  code/sum_entrypoint.py\n",
    "! gzip  model.tar model.tar.gz\n",
    "! rm model.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step before we're ready to deploy is to upload the model artifact and fetch the ECR uri for the huggingface inference container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_data = sess.upload_data(\"model.tar.gz\", bucket, key_prefix)\n",
    "inference_image_uri = image_uris.retrieve(\n",
    "            \"huggingface\",\n",
    "            region,\n",
    "            version=\"4.6.1\",\n",
    "            py_version=\"py36\",\n",
    "            instance_type=\"ml.m5.xlarge\",\n",
    "            image_scope=\"inference\",\n",
    "            base_framework_version=\"pytorch1.8.1\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "The model is now ready to be deployed. The first step is to create a SageMaker Model Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"document-summarization\"\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_model_data,\n",
    "        \"Environment\": {\n",
    "            \"SAGEMAKER_PROGRAM\": \"sum_entrypoint.py\",\n",
    "            \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",\n",
    "            \"SAGEMAKER_REGION\": region,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the endpoint configuration with asynch inference enabled. We can optionally provide an SNS topic to send a comletion notification for each inference request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.m5.xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "        }\n",
    "    ],\n",
    "    AsyncInferenceConfig={\n",
    "        \"OutputConfig\": {\n",
    "            \"S3OutputPath\": f\"s3://{bucket}/{key_prefix}/async-output\",\n",
    "            # Optionally specify Amazon SNS topics\n",
    "            # \"NotificationConfig\": {\n",
    "            #   \"SuccessTopic\": \"arn:aws:sns:us-east-2:123456789012:MyTopic\",\n",
    "            #   \"ErrorTopic\": \"arn:aws:sns:us-east-2:123456789012:MyTopic\",\n",
    "            # }\n",
    "        },\n",
    "        \"ClientConfig\": {\"MaxConcurrentInvocationsPerInstance\": 4},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note you may need to add ECR permissions such as `AmazonEC2ContainerRegistryReadOnly` to your execution role if your endpoint fails to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = sm_client.get_waiter(\"endpoint_in_service\")\n",
    "print(\"Waiting for endpoint to create...\")\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "print(f\"Endpoint Status: {resp['EndpointStatus']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll upload some sample articles from the data directory to test our endpoint. Feel free to add your own text documents as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive ./data s3://{bucket}/{key_prefix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the endpoint with one of the uploaded documents\n",
    "response = smr_client.invoke_endpoint_async(\n",
    "    EndpointName=endpoint_name, \n",
    "    InputLocation=f\"s3://{bucket}/{key_prefix}/article1.txt\",\n",
    "    ContentType=\"text/csv\"\n",
    ")\n",
    "output_key = \"/\".join(response[\"OutputLocation\"].split(\"/\")[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait until the results become available\n",
    "\n",
    "from botocore.errorfactory import ClientError\n",
    "\n",
    "total_sleep = 0\n",
    "\n",
    "while True:\n",
    "    try: \n",
    "        # check if result is ready \n",
    "        s3_client.head_object(Bucket=bucket, Key=output_key)\n",
    "        result = sess.read_s3_file(bucket=bucket, key_prefix=output_key)\n",
    "        print(\"Results are ready\")\n",
    "        break\n",
    "    except ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"404\":\n",
    "            if total_sleep < 60:\n",
    "                print(\"Results are not yet ready. Sleeping for 5s\")\n",
    "                time.sleep(5)\n",
    "                total_sleep+=5\n",
    "                continue\n",
    "            else:\n",
    "                print(\"Been waiting for 60s, terminating the poll. Check the endpoint logs to see if there are any issues\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Unexpected error encountered. Please review the Cloud Watch logs for {endpoint_name}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)\n",
    "! rm model.tar.gz\n",
    "! aws s3 rm s3://{bucket}/{key_prefix} --recursive > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
